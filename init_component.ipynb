{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0oFJQATW882","executionInfo":{"status":"ok","timestamp":1687075958109,"user_tz":-420,"elapsed":22273,"user":{"displayName":"Yamada Shiro","userId":"07289017466336740839"}},"outputId":"99e62d05-aea6-409b-fe35-7ddb957816f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# %cd /content/gdrive/MyDrive/KLTN/source\n","# !git clone https://github.com/clovaai/voxceleb_trainer"],"metadata":{"id":"uifjdciXXVUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/KLTN/source/voxceleb_trainer_component/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acDDZvwIYpYb","executionInfo":{"status":"ok","timestamp":1687075958824,"user_tz":-420,"elapsed":717,"user":{"displayName":"Yamada Shiro","userId":"07289017466336740839"}},"outputId":"f114cbbe-413f-40ac-ac09-29923d00c114"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/19nyuDPQ1eh6564W5mYp2mmfVYxIS9Gyu/KLTN/source/voxceleb_trainer_component\n"]}]},{"cell_type":"code","source":["!pip install asteroid_filterbanks -q\n","!pip install soundfile==0.10.3.post1 -q"],"metadata":{"id":"hBp3IYa4Znnx","executionInfo":{"status":"ok","timestamp":1687076486659,"user_tz":-420,"elapsed":13836,"user":{"displayName":"Yamada Shiro","userId":"07289017466336740839"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6ea24172-7cdf-4a02-c46a-8d8d00eadf54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","librosa 0.10.0.post2 requires soundfile>=0.12.1, but you have soundfile 0.10.3.post1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!python ./trainSpeakerNet.py --config ./configs/RawNet3_AAM.yaml --train_list ../../dataset/zalo_dataset/dataset_fix/train_list.txt --train_path ../../dataset/zalo_dataset/dataset_fix/train --test_list ../../dataset/zalo_dataset/dataset_fix/veri_val.txt --test_path ../../dataset/zalo_dataset/dataset_fix/val --initial_model ../rawnet3/model.pt --max_epoch 60 --save_path /content/gdrive/MyDrive/KLTN/source/voxceleb_trainer_component/exps_component"],"metadata":{"id":"nDbiFJYJhorW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8220fbb-c0c8-4835-cd30-15d728b14b7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python Version: 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]\n","PyTorch Version: 2.0.1+cu118\n","Number of GPUs: 1\n","Save path: exps/RawNet3_AAM\n","self.encoder_type ECA\n","Initialised AAMSoftmax margin 0.100 scale 30.000\n","Initialised Adam optimizer\n","Initialised step LR scheduler\n","Model ../rawnet3/model.pt loaded!\n","Processing 8000 of 8000:Loss 3.409056 TEER/TAcc 44.812% - 1.24 Hz \n"," 2023-06-18 10:15:38 Epoch 1, TEER/TAcc 44.81, TLOSS 3.409056, LR 0.001000\n","Reading 900 of 973: 6.03 Hz, embedding size 256\n","Computing 19900 of 20000: 2933.42 Hz\n"," 2023-06-18 10:18:25 Epoch 1, VEER 6.6000, MinDCF 0.35480\n","Processing 8064 of 8064:Loss 1.300100 TEER/TAcc 71.987% - 69.79 Hz \n"," 2023-06-18 10:26:46 Epoch 2, TEER/TAcc 71.99, TLOSS 1.300100, LR 0.001000\n","Reading 900 of 973: 6.22 Hz, embedding size 256\n","Computing 19900 of 20000: 3016.47 Hz\n"," 2023-06-18 10:29:29 Epoch 2, VEER 6.5900, MinDCF 0.33630\n","Processing 8032 of 8032:Loss 1.026781 TEER/TAcc 76.008% - 71.27 Hz \n"," 2023-06-18 10:31:40 Epoch 3, TEER/TAcc 76.01, TLOSS 1.026781, LR 0.001000\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 2991.39 Hz\n"," 2023-06-18 10:34:23 Epoch 3, VEER 6.7600, MinDCF 0.34950\n","Processing 8000 of 8000:Loss 0.917998 TEER/TAcc 77.575% - 71.48 Hz \n"," 2023-06-18 10:36:20 Epoch 4, TEER/TAcc 77.58, TLOSS 0.917998, LR 0.001000\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 2938.33 Hz\n"," 2023-06-18 10:39:02 Epoch 4, VEER 6.4400, MinDCF 0.32950\n","Processing 8032 of 8032:Loss 0.847696 TEER/TAcc 79.208% - 72.18 Hz \n"," 2023-06-18 10:40:58 Epoch 5, TEER/TAcc 79.21, TLOSS 0.847696, LR 0.001000\n","Reading 900 of 973: 6.22 Hz, embedding size 256\n","Computing 19900 of 20000: 3436.72 Hz\n"," 2023-06-18 10:43:41 Epoch 5, VEER 6.7600, MinDCF 0.33140\n","Processing 8000 of 8000:Loss 0.785276 TEER/TAcc 79.838% - 71.14 Hz \n"," 2023-06-18 10:45:36 Epoch 6, TEER/TAcc 79.84, TLOSS 0.785276, LR 0.001000\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3324.03 Hz\n"," 2023-06-18 10:48:18 Epoch 6, VEER 6.8600, MinDCF 0.36000\n","Processing 8032 of 8032:Loss 0.758669 TEER/TAcc 80.466% - 70.98 Hz \n"," 2023-06-18 10:50:14 Epoch 7, TEER/TAcc 80.47, TLOSS 0.758669, LR 0.001000\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 2828.96 Hz\n"," 2023-06-18 10:52:57 Epoch 7, VEER 6.8200, MinDCF 0.35800\n","Processing 8032 of 8032:Loss 0.729359 TEER/TAcc 80.453% - 71.12 Hz \n"," 2023-06-18 10:54:54 Epoch 8, TEER/TAcc 80.45, TLOSS 0.729359, LR 0.001000\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3248.94 Hz\n"," 2023-06-18 10:57:36 Epoch 8, VEER 6.4500, MinDCF 0.32720\n","Processing 8032 of 8032:Loss 0.686300 TEER/TAcc 80.789% - 71.42 Hz \n"," 2023-06-18 10:59:31 Epoch 9, TEER/TAcc 80.79, TLOSS 0.686300, LR 0.001000\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3398.85 Hz\n"," 2023-06-18 11:02:13 Epoch 9, VEER 6.8400, MinDCF 0.32910\n","Processing 8032 of 8032:Loss 0.676586 TEER/TAcc 81.499% - 71.53 Hz \n"," 2023-06-18 11:04:09 Epoch 10, TEER/TAcc 81.50, TLOSS 0.676586, LR 0.001000\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 2949.73 Hz\n"," 2023-06-18 11:06:52 Epoch 10, VEER 6.2600, MinDCF 0.34130\n","Processing 8032 of 8032:Loss 0.593792 TEER/TAcc 82.707% - 71.87 Hz \n"," 2023-06-18 11:08:48 Epoch 11, TEER/TAcc 82.71, TLOSS 0.593792, LR 0.000750\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3320.65 Hz\n"," 2023-06-18 11:11:30 Epoch 11, VEER 6.3300, MinDCF 0.31720\n","Processing 8032 of 8032:Loss 0.528144 TEER/TAcc 84.674% - 70.92 Hz \n"," 2023-06-18 11:13:26 Epoch 12, TEER/TAcc 84.67, TLOSS 0.528144, LR 0.000750\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3427.95 Hz\n"," 2023-06-18 11:16:07 Epoch 12, VEER 6.3500, MinDCF 0.33860\n","Processing 8096 of 8096:Loss 0.539302 TEER/TAcc 84.153% - 71.86 Hz \n"," 2023-06-18 11:18:05 Epoch 13, TEER/TAcc 84.15, TLOSS 0.539302, LR 0.000750\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3242.64 Hz\n"," 2023-06-18 11:20:47 Epoch 13, VEER 6.3100, MinDCF 0.31210\n","Processing 8000 of 8000:Loss 0.540257 TEER/TAcc 84.412% - 71.99 Hz \n"," 2023-06-18 11:22:42 Epoch 14, TEER/TAcc 84.41, TLOSS 0.540257, LR 0.000750\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 2960.93 Hz\n"," 2023-06-18 11:25:25 Epoch 14, VEER 6.2500, MinDCF 0.30550\n","Processing 8032 of 8032:Loss 0.529154 TEER/TAcc 84.263% - 71.82 Hz \n"," 2023-06-18 11:27:20 Epoch 15, TEER/TAcc 84.26, TLOSS 0.529154, LR 0.000750\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3442.16 Hz\n"," 2023-06-18 11:30:02 Epoch 15, VEER 6.7800, MinDCF 0.32350\n","Processing 8032 of 8032:Loss 0.547251 TEER/TAcc 83.952% - 71.84 Hz \n"," 2023-06-18 11:32:00 Epoch 16, TEER/TAcc 83.95, TLOSS 0.547251, LR 0.000750\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3376.38 Hz\n"," 2023-06-18 11:34:42 Epoch 16, VEER 6.4000, MinDCF 0.32400\n","Processing 8032 of 8032:Loss 0.548576 TEER/TAcc 83.752% - 71.34 Hz \n"," 2023-06-18 11:36:38 Epoch 17, TEER/TAcc 83.75, TLOSS 0.548576, LR 0.000750\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3526.69 Hz\n"," 2023-06-18 11:39:20 Epoch 17, VEER 6.2700, MinDCF 0.32130\n","Processing 8032 of 8032:Loss 0.523226 TEER/TAcc 84.213% - 71.60 Hz \n"," 2023-06-18 11:41:17 Epoch 18, TEER/TAcc 84.21, TLOSS 0.523226, LR 0.000750\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3322.35 Hz\n"," 2023-06-18 11:43:59 Epoch 18, VEER 6.8000, MinDCF 0.34140\n","Processing 8064 of 8064:Loss 0.538527 TEER/TAcc 84.313% - 71.89 Hz \n"," 2023-06-18 11:45:56 Epoch 19, TEER/TAcc 84.31, TLOSS 0.538527, LR 0.000750\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 2973.56 Hz\n"," 2023-06-18 11:48:38 Epoch 19, VEER 6.3900, MinDCF 0.32050\n","Processing 8000 of 8000:Loss 0.528782 TEER/TAcc 84.312% - 70.85 Hz \n"," 2023-06-18 11:50:34 Epoch 20, TEER/TAcc 84.31, TLOSS 0.528782, LR 0.000750\n","Reading 900 of 973: 6.22 Hz, embedding size 256\n","Computing 19900 of 20000: 3310.85 Hz\n"," 2023-06-18 11:53:16 Epoch 20, VEER 6.2300, MinDCF 0.32470\n","Processing 8000 of 8000:Loss 0.472586 TEER/TAcc 84.925% - 71.60 Hz \n"," 2023-06-18 11:55:11 Epoch 21, TEER/TAcc 84.92, TLOSS 0.472586, LR 0.000563\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3456.66 Hz\n"," 2023-06-18 11:57:53 Epoch 21, VEER 6.7200, MinDCF 0.32960\n","Processing 8032 of 8032:Loss 0.467773 TEER/TAcc 85.321% - 71.87 Hz \n"," 2023-06-18 11:59:47 Epoch 22, TEER/TAcc 85.32, TLOSS 0.467773, LR 0.000563\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 2999.84 Hz\n"," 2023-06-18 12:02:30 Epoch 22, VEER 6.3200, MinDCF 0.32780\n","Processing 8064 of 8064:Loss 0.445598 TEER/TAcc 85.875% - 69.13 Hz \n"," 2023-06-18 12:04:25 Epoch 23, TEER/TAcc 85.88, TLOSS 0.445598, LR 0.000563\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3448.74 Hz\n"," 2023-06-18 12:07:06 Epoch 23, VEER 6.4500, MinDCF 0.31070\n","Processing 8000 of 8000:Loss 0.459569 TEER/TAcc 85.425% - 70.28 Hz \n"," 2023-06-18 12:09:01 Epoch 24, TEER/TAcc 85.42, TLOSS 0.459569, LR 0.000563\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3152.38 Hz\n"," 2023-06-18 12:11:43 Epoch 24, VEER 6.6900, MinDCF 0.31030\n","Processing 8032 of 8032:Loss 0.460170 TEER/TAcc 85.757% - 70.56 Hz \n"," 2023-06-18 12:13:37 Epoch 25, TEER/TAcc 85.76, TLOSS 0.460170, LR 0.000563\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3249.48 Hz\n"," 2023-06-18 12:16:19 Epoch 25, VEER 6.3900, MinDCF 0.32100\n","Processing 8032 of 8032:Loss 0.464724 TEER/TAcc 85.284% - 71.02 Hz \n"," 2023-06-18 12:18:13 Epoch 26, TEER/TAcc 85.28, TLOSS 0.464724, LR 0.000563\n","Reading 900 of 973: 6.26 Hz, embedding size 256\n","Computing 19900 of 20000: 3397.45 Hz\n"," 2023-06-18 12:20:55 Epoch 26, VEER 6.1400, MinDCF 0.30300\n","Processing 8032 of 8032:Loss 0.461493 TEER/TAcc 85.097% - 72.10 Hz \n"," 2023-06-18 12:22:47 Epoch 27, TEER/TAcc 85.10, TLOSS 0.461493, LR 0.000563\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3164.98 Hz\n"," 2023-06-18 12:25:29 Epoch 27, VEER 6.0600, MinDCF 0.32740\n","Processing 8032 of 8032:Loss 0.451515 TEER/TAcc 85.894% - 71.59 Hz \n"," 2023-06-18 12:27:22 Epoch 28, TEER/TAcc 85.89, TLOSS 0.451515, LR 0.000563\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3538.87 Hz\n"," 2023-06-18 12:30:04 Epoch 28, VEER 6.3100, MinDCF 0.32600\n","Processing 8032 of 8032:Loss 0.454221 TEER/TAcc 85.421% - 71.81 Hz \n"," 2023-06-18 12:31:56 Epoch 29, TEER/TAcc 85.42, TLOSS 0.454221, LR 0.000563\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3169.43 Hz\n"," 2023-06-18 12:34:39 Epoch 29, VEER 6.4900, MinDCF 0.34520\n","Processing 8032 of 8032:Loss 0.447751 TEER/TAcc 85.844% - 71.95 Hz \n"," 2023-06-18 12:36:31 Epoch 30, TEER/TAcc 85.84, TLOSS 0.447751, LR 0.000563\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3260.19 Hz\n"," 2023-06-18 12:39:13 Epoch 30, VEER 6.1800, MinDCF 0.33330\n","Processing 8032 of 8032:Loss 0.413454 TEER/TAcc 86.243% - 70.77 Hz \n"," 2023-06-18 12:41:05 Epoch 31, TEER/TAcc 86.24, TLOSS 0.413454, LR 0.000422\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3402.11 Hz\n"," 2023-06-18 12:43:47 Epoch 31, VEER 5.7100, MinDCF 0.31030\n","Processing 8032 of 8032:Loss 0.399855 TEER/TAcc 86.815% - 72.06 Hz \n"," 2023-06-18 12:45:40 Epoch 32, TEER/TAcc 86.82, TLOSS 0.399855, LR 0.000422\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3101.84 Hz\n"," 2023-06-18 12:48:22 Epoch 32, VEER 5.9700, MinDCF 0.32220\n","Processing 8032 of 8032:Loss 0.399001 TEER/TAcc 86.990% - 71.33 Hz \n"," 2023-06-18 12:50:14 Epoch 33, TEER/TAcc 86.99, TLOSS 0.399001, LR 0.000422\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3420.15 Hz\n"," 2023-06-18 12:52:56 Epoch 33, VEER 6.0600, MinDCF 0.31640\n","Processing 8032 of 8032:Loss 0.401925 TEER/TAcc 86.678% - 71.41 Hz \n"," 2023-06-18 12:54:48 Epoch 34, TEER/TAcc 86.68, TLOSS 0.401925, LR 0.000422\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3058.39 Hz\n"," 2023-06-18 12:57:31 Epoch 34, VEER 6.2100, MinDCF 0.32070\n","Processing 8064 of 8064:Loss 0.392621 TEER/TAcc 86.706% - 72.06 Hz \n"," 2023-06-18 12:59:23 Epoch 35, TEER/TAcc 86.71, TLOSS 0.392621, LR 0.000422\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3333.49 Hz\n"," 2023-06-18 13:02:05 Epoch 35, VEER 6.3100, MinDCF 0.32010\n","Processing 8032 of 8032:Loss 0.401491 TEER/TAcc 86.367% - 71.74 Hz \n"," 2023-06-18 13:03:58 Epoch 36, TEER/TAcc 86.37, TLOSS 0.401491, LR 0.000422\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 2966.37 Hz\n"," 2023-06-18 13:06:40 Epoch 36, VEER 6.1000, MinDCF 0.32620\n","Processing 8000 of 8000:Loss 0.395853 TEER/TAcc 87.300% - 72.41 Hz \n"," 2023-06-18 13:08:32 Epoch 37, TEER/TAcc 87.30, TLOSS 0.395853, LR 0.000422\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3445.14 Hz\n"," 2023-06-18 13:11:13 Epoch 37, VEER 6.0500, MinDCF 0.31940\n","Processing 8064 of 8064:Loss 0.398959 TEER/TAcc 86.582% - 71.17 Hz \n"," 2023-06-18 13:13:06 Epoch 38, TEER/TAcc 86.58, TLOSS 0.398959, LR 0.000422\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3003.77 Hz\n"," 2023-06-18 13:15:48 Epoch 38, VEER 6.2300, MinDCF 0.31880\n","Processing 8032 of 8032:Loss 0.399095 TEER/TAcc 86.877% - 71.96 Hz \n"," 2023-06-18 13:17:41 Epoch 39, TEER/TAcc 86.88, TLOSS 0.399095, LR 0.000422\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3447.69 Hz\n"," 2023-06-18 13:20:22 Epoch 39, VEER 6.1200, MinDCF 0.31520\n","Processing 8000 of 8000:Loss 0.389796 TEER/TAcc 87.037% - 71.73 Hz \n"," 2023-06-18 13:22:14 Epoch 40, TEER/TAcc 87.04, TLOSS 0.389796, LR 0.000422\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3050.14 Hz\n"," 2023-06-18 13:24:56 Epoch 40, VEER 6.1300, MinDCF 0.32010\n","Processing 8032 of 8032:Loss 0.380372 TEER/TAcc 86.952% - 70.46 Hz \n"," 2023-06-18 13:26:49 Epoch 41, TEER/TAcc 86.95, TLOSS 0.380372, LR 0.000316\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 3434.93 Hz\n"," 2023-06-18 13:29:30 Epoch 41, VEER 6.0700, MinDCF 0.31340\n","Processing 8032 of 8032:Loss 0.357269 TEER/TAcc 87.562% - 71.09 Hz \n"," 2023-06-18 13:31:23 Epoch 42, TEER/TAcc 87.56, TLOSS 0.357269, LR 0.000316\n","Reading 900 of 973: 6.25 Hz, embedding size 256\n","Computing 19900 of 20000: 2966.48 Hz\n"," 2023-06-18 13:34:05 Epoch 42, VEER 6.3800, MinDCF 0.32260\n","Processing 8064 of 8064:Loss 0.360577 TEER/TAcc 87.599% - 71.69 Hz \n"," 2023-06-18 13:35:58 Epoch 43, TEER/TAcc 87.60, TLOSS 0.360577, LR 0.000316\n","Reading 900 of 973: 6.22 Hz, embedding size 256\n","Computing 19900 of 20000: 3440.51 Hz\n"," 2023-06-18 13:38:40 Epoch 43, VEER 6.3500, MinDCF 0.33450\n","Processing 8064 of 8064:Loss 0.361479 TEER/TAcc 87.661% - 71.67 Hz \n"," 2023-06-18 13:40:33 Epoch 44, TEER/TAcc 87.66, TLOSS 0.361479, LR 0.000316\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 2984.53 Hz\n"," 2023-06-18 13:43:16 Epoch 44, VEER 6.2200, MinDCF 0.30370\n","Processing 8064 of 8064:Loss 0.365971 TEER/TAcc 87.252% - 72.05 Hz \n"," 2023-06-18 13:45:09 Epoch 45, TEER/TAcc 87.25, TLOSS 0.365971, LR 0.000316\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3450.79 Hz\n"," 2023-06-18 13:47:51 Epoch 45, VEER 6.3000, MinDCF 0.31810\n","Processing 8000 of 8000:Loss 0.371451 TEER/TAcc 87.475% - 71.80 Hz \n"," 2023-06-18 13:49:43 Epoch 46, TEER/TAcc 87.47, TLOSS 0.371451, LR 0.000316\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 2973.97 Hz\n"," 2023-06-18 13:52:26 Epoch 46, VEER 6.1600, MinDCF 0.32860\n","Processing 8032 of 8032:Loss 0.353451 TEER/TAcc 87.874% - 71.28 Hz \n"," 2023-06-18 13:54:20 Epoch 47, TEER/TAcc 87.87, TLOSS 0.353451, LR 0.000316\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3433.28 Hz\n"," 2023-06-18 13:57:02 Epoch 47, VEER 5.9300, MinDCF 0.32820\n","Processing 8032 of 8032:Loss 0.364626 TEER/TAcc 87.226% - 70.75 Hz \n"," 2023-06-18 13:58:57 Epoch 48, TEER/TAcc 87.23, TLOSS 0.364626, LR 0.000316\n","Reading 900 of 973: 6.23 Hz, embedding size 256\n","Computing 19900 of 20000: 3122.50 Hz\n"," 2023-06-18 14:01:39 Epoch 48, VEER 6.4000, MinDCF 0.32860\n","Processing 7520 of 8032:Loss 0.361944 TEER/TAcc 87.806% - 71.17 Hz "]}]},{"cell_type":"code","source":[],"metadata":{"id":"mlHT0iSXoIel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pJPFIvoAoIbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KhemuiOgoIZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from asteroid_filterbanks import Encoder, ParamSincFB\n"],"metadata":{"id":"ZeH24IXknKO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class PreEmphasis(torch.nn.Module):\n","    def __init__(self, coef: float = 0.97) -> None:\n","        super().__init__()\n","        self.coef = coef\n","        # make kernel\n","        # In pytorch, the convolution operation uses cross-correlation. So, filter is flipped.\n","        self.register_buffer(\n","            \"flipped_filter\",\n","            torch.FloatTensor([-self.coef, 1.0]).unsqueeze(0).unsqueeze(0),\n","        )\n","\n","    def forward(self, input: torch.tensor) -> torch.tensor:\n","        assert (\n","            len(input.size()) == 2\n","        ), \"The number of dimensions of input tensor must be 2!\"\n","        # reflect padding to match lengths of in/out\n","        input = input.unsqueeze(1)\n","        input = F.pad(input, (1, 0), \"reflect\")\n","        return F.conv1d(input, self.flipped_filter)\n","\n","\n","class AFMS(nn.Module):\n","    \"\"\"\n","    Alpha-Feature map scaling, added to the output of each residual block[1,2].\n","\n","    Reference:\n","    [1] RawNet2 : https://www.isca-speech.org/archive/Interspeech_2020/pdfs/1011.pdf\n","    [2] AMFS    : https://www.koreascience.or.kr/article/JAKO202029757857763.page\n","    \"\"\"\n","\n","    def __init__(self, nb_dim: int) -> None:\n","        super().__init__()\n","        self.alpha = nn.Parameter(torch.ones((nb_dim, 1)))\n","        self.fc = nn.Linear(nb_dim, nb_dim)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        y = F.adaptive_avg_pool1d(x, 1).view(x.size(0), -1)\n","        y = self.sig(self.fc(y)).view(x.size(0), x.size(1), -1)\n","\n","        x = x + self.alpha\n","        x = x * y\n","        return x\n","\n","\n","class Bottle2neck(nn.Module):\n","    def __init__(\n","        self,\n","        inplanes,\n","        planes,\n","        kernel_size=None,\n","        dilation=None,\n","        scale=4,\n","        pool=False,\n","    ):\n","\n","        super().__init__()\n","\n","        width = int(math.floor(planes / scale))\n","\n","        self.conv1 = nn.Conv1d(inplanes, width * scale, kernel_size=1)\n","        self.bn1 = nn.BatchNorm1d(width * scale)\n","\n","        self.nums = scale - 1\n","\n","        convs = []\n","        bns = []\n","\n","        num_pad = math.floor(kernel_size / 2) * dilation\n","\n","        for i in range(self.nums):\n","            convs.append(\n","                nn.Conv1d(\n","                    width,\n","                    width,\n","                    kernel_size=kernel_size,\n","                    dilation=dilation,\n","                    padding=num_pad,\n","                )\n","            )\n","            bns.append(nn.BatchNorm1d(width))\n","\n","        self.convs = nn.ModuleList(convs)\n","        self.bns = nn.ModuleList(bns)\n","\n","        self.conv3 = nn.Conv1d(width * scale, planes, kernel_size=1)\n","        self.bn3 = nn.BatchNorm1d(planes)\n","\n","        self.relu = nn.ReLU()\n","\n","        self.width = width\n","\n","        self.mp = nn.MaxPool1d(pool) if pool else False\n","        self.afms = AFMS(planes)\n","\n","        if inplanes != planes:  # if change in number of filters\n","            self.residual = nn.Sequential(\n","                nn.Conv1d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n","            )\n","        else:\n","            self.residual = nn.Identity()\n","\n","    def forward(self, x):\n","        residual = self.residual(x)\n","\n","        out = self.conv1(x)\n","        out = self.relu(out)\n","        out = self.bn1(out)\n","\n","        spx = torch.split(out, self.width, 1)\n","        for i in range(self.nums):\n","            if i == 0:\n","                sp = spx[i]\n","            else:\n","                sp = sp + spx[i]\n","            sp = self.convs[i](sp)\n","            sp = self.relu(sp)\n","            sp = self.bns[i](sp)\n","            if i == 0:\n","                out = sp\n","            else:\n","                out = torch.cat((out, sp), 1)\n","\n","        out = torch.cat((out, spx[self.nums]), 1)\n","\n","        out = self.conv3(out)\n","        out = self.relu(out)\n","        out = self.bn3(out)\n","\n","        out += residual\n","        if self.mp:\n","            out = self.mp(out)\n","        out = self.afms(out)\n","\n","        return out\n"],"metadata":{"id":"mX7fH-OWnQw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RawNet3(nn.Module):\n","    def __init__(self, block, model_scale, context, summed, C=1024, **kwargs):\n","        super().__init__()\n","\n","        nOut = kwargs[\"nOut\"]\n","\n","        self.context = context\n","        self.encoder_type = kwargs[\"encoder_type\"]\n","        self.log_sinc = kwargs[\"log_sinc\"]\n","        self.norm_sinc = kwargs[\"norm_sinc\"]\n","        self.out_bn = kwargs[\"out_bn\"]\n","        self.summed = summed\n","\n","        self.preprocess = nn.Sequential(\n","            PreEmphasis(), nn.InstanceNorm1d(1, eps=1e-4, affine=True)\n","        )\n","        self.conv1 = Encoder(\n","            ParamSincFB(\n","                C // 4,\n","                251,\n","                stride=kwargs[\"sinc_stride\"],\n","            )\n","        )\n","        self.relu = nn.ReLU()\n","        self.bn1 = nn.BatchNorm1d(C // 4)\n","\n","        self.layer1 = block(\n","            C // 4, C, kernel_size=3, dilation=2, scale=model_scale, pool=5\n","        )\n","        self.layer2 = block(\n","            C, C, kernel_size=3, dilation=3, scale=model_scale, pool=3\n","        )\n","        self.layer3 = block(C, C, kernel_size=3, dilation=4, scale=model_scale)\n","        self.layer4 = nn.Conv1d(3 * C, 1536, kernel_size=1)\n","\n","        if self.context:\n","            attn_input = 1536 * 3\n","        else:\n","            attn_input = 1536\n","        print(\"self.encoder_type\", self.encoder_type)\n","        if self.encoder_type == \"ECA\":\n","            attn_output = 1536\n","        elif self.encoder_type == \"ASP\":\n","            attn_output = 1\n","        else:\n","            raise ValueError(\"Undefined encoder\")\n","\n","        self.attention = nn.Sequential(\n","            nn.Conv1d(attn_input, 128, kernel_size=1),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(128),\n","            nn.Conv1d(128, attn_output, kernel_size=1),\n","            nn.Softmax(dim=2),\n","        )\n","\n","        self.bn5 = nn.BatchNorm1d(3072)\n","\n","        self.fc6 = nn.Linear(3072, nOut)\n","        self.bn6 = nn.BatchNorm1d(nOut)\n","\n","        self.mp3 = nn.MaxPool1d(3)\n","\n","        self.new_component = nn.Sequential(\n","            nn.Linear(256, 256),\n","            nn.Linear(256, 256),\n","            nn.Linear(256, 256),\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: input mini-batch (bs, samp)\n","        \"\"\"\n","\n","        with torch.cuda.amp.autocast(enabled=False):\n","            x = self.preprocess(x)\n","            x = torch.abs(self.conv1(x))\n","            if self.log_sinc:\n","                x = torch.log(x + 1e-6)\n","            if self.norm_sinc == \"mean\":\n","                x = x - torch.mean(x, dim=-1, keepdim=True)\n","            elif self.norm_sinc == \"mean_std\":\n","                m = torch.mean(x, dim=-1, keepdim=True)\n","                s = torch.std(x, dim=-1, keepdim=True)\n","                s[s < 0.001] = 0.001\n","                x = (x - m) / s\n","\n","        if self.summed:\n","            x1 = self.layer1(x)\n","            x2 = self.layer2(x1)\n","            x3 = self.layer3(self.mp3(x1) + x2)\n","        else:\n","            x1 = self.layer1(x)\n","            x2 = self.layer2(x1)\n","            x3 = self.layer3(x2)\n","\n","        x = self.layer4(torch.cat((self.mp3(x1), x2, x3), dim=1))\n","        x = self.relu(x)\n","\n","        t = x.size()[-1]\n","\n","        if self.context:\n","            global_x = torch.cat(\n","                (\n","                    x,\n","                    torch.mean(x, dim=2, keepdim=True).repeat(1, 1, t),\n","                    torch.sqrt(\n","                        torch.var(x, dim=2, keepdim=True).clamp(\n","                            min=1e-4, max=1e4\n","                        )\n","                    ).repeat(1, 1, t),\n","                ),\n","                dim=1,\n","            )\n","        else:\n","            global_x = x\n","\n","        w = self.attention(global_x)\n","\n","        mu = torch.sum(x * w, dim=2)\n","        sg = torch.sqrt(\n","            (torch.sum((x**2) * w, dim=2) - mu**2).clamp(min=1e-4, max=1e4)\n","        )\n","\n","        x = torch.cat((mu, sg), 1)\n","\n","        x = self.bn5(x)\n","\n","        x = self.fc6(x)\n","\n","        if self.out_bn:\n","            x = self.bn6(x)\n","        x = self.new_component(x)\n","        return x\n","\n"],"metadata":{"id":"JHGxPWSlnBvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RawNet3(\n","        Bottle2neck,\n","        model_scale=8,\n","        context=True,\n","        summed=True,\n","        encoder_type=\"ECA\",\n","        nOut=256,\n","        out_bn=False,\n","        sinc_stride=10,\n","        log_sinc=True,\n","        norm_sinc=\"mean\",\n","        grad_mult=1,\n","    )\n","for param in model.parameters():\n","  param.requires_grad = False\n","for param in model.new_component.parameters():\n","  param.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMUSZFlzm96Y","executionInfo":{"status":"ok","timestamp":1687076863391,"user_tz":-420,"elapsed":6,"user":{"displayName":"Yamada Shiro","userId":"07289017466336740839"}},"outputId":"74a208f3-2150-4b53-9197-1ab17e45699c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["self.encoder_type ECA\n"]}]},{"cell_type":"code","source":["!pip install torchinfo -q\n","from torchinfo import summary\n","summary(model, input_size=(1, 48000)) # component"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9iZ0JEbnUfR","executionInfo":{"status":"ok","timestamp":1687076906313,"user_tz":-420,"elapsed":15250,"user":{"displayName":"Yamada Shiro","userId":"07289017466336740839"}},"outputId":"241b1b07-ef94-4a21-bd15-bad2a590d5bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","RawNet3                                  [1, 256]                  1,024\n","├─Sequential: 1-1                        [1, 1, 48000]             --\n","│    └─PreEmphasis: 2-1                  [1, 1, 48000]             --\n","│    └─InstanceNorm1d: 2-2               [1, 1, 48000]             (2)\n","├─Encoder: 1-2                           [1, 256, 4775]            256\n","├─Bottle2neck: 1-3                       [1, 1024, 955]            --\n","│    └─Sequential: 2-3                   [1, 1024, 4775]           --\n","│    │    └─Conv1d: 3-1                  [1, 1024, 4775]           (262,144)\n","│    └─Conv1d: 2-4                       [1, 1024, 4775]           (263,168)\n","│    └─ReLU: 2-5                         [1, 1024, 4775]           --\n","│    └─BatchNorm1d: 2-6                  [1, 1024, 4775]           (2,048)\n","│    └─ModuleList: 2-25                  --                        (recursive)\n","│    │    └─Conv1d: 3-2                  [1, 128, 4775]            (49,280)\n","│    └─ReLU: 2-8                         [1, 128, 4775]            --\n","│    └─ModuleList: 2-27                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-3             [1, 128, 4775]            (256)\n","│    └─ModuleList: 2-25                  --                        (recursive)\n","│    │    └─Conv1d: 3-4                  [1, 128, 4775]            (49,280)\n","│    └─ReLU: 2-11                        [1, 128, 4775]            --\n","│    └─ModuleList: 2-27                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-5             [1, 128, 4775]            (256)\n","│    └─ModuleList: 2-25                  --                        (recursive)\n","│    │    └─Conv1d: 3-6                  [1, 128, 4775]            (49,280)\n","│    └─ReLU: 2-14                        [1, 128, 4775]            --\n","│    └─ModuleList: 2-27                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-7             [1, 128, 4775]            (256)\n","│    └─ModuleList: 2-25                  --                        (recursive)\n","│    │    └─Conv1d: 3-8                  [1, 128, 4775]            (49,280)\n","│    └─ReLU: 2-17                        [1, 128, 4775]            --\n","│    └─ModuleList: 2-27                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-9             [1, 128, 4775]            (256)\n","│    └─ModuleList: 2-25                  --                        (recursive)\n","│    │    └─Conv1d: 3-10                 [1, 128, 4775]            (49,280)\n","│    └─ReLU: 2-20                        [1, 128, 4775]            --\n","│    └─ModuleList: 2-27                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-11            [1, 128, 4775]            (256)\n","│    └─ModuleList: 2-25                  --                        (recursive)\n","│    │    └─Conv1d: 3-12                 [1, 128, 4775]            (49,280)\n","│    └─ReLU: 2-23                        [1, 128, 4775]            --\n","│    └─ModuleList: 2-27                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-13            [1, 128, 4775]            (256)\n","│    └─ModuleList: 2-25                  --                        (recursive)\n","│    │    └─Conv1d: 3-14                 [1, 128, 4775]            (49,280)\n","│    └─ReLU: 2-26                        [1, 128, 4775]            --\n","│    └─ModuleList: 2-27                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-15            [1, 128, 4775]            (256)\n","│    └─Conv1d: 2-28                      [1, 1024, 4775]           (1,049,600)\n","│    └─ReLU: 2-29                        [1, 1024, 4775]           --\n","│    └─BatchNorm1d: 2-30                 [1, 1024, 4775]           (2,048)\n","│    └─MaxPool1d: 2-31                   [1, 1024, 955]            --\n","│    └─AFMS: 2-32                        [1, 1024, 955]            1,024\n","│    │    └─Linear: 3-16                 [1, 1024]                 (1,049,600)\n","│    │    └─Sigmoid: 3-17                [1, 1024]                 --\n","├─Bottle2neck: 1-4                       [1, 1024, 318]            --\n","│    └─Identity: 2-33                    [1, 1024, 955]            --\n","│    └─Conv1d: 2-34                      [1, 1024, 955]            (1,049,600)\n","│    └─ReLU: 2-35                        [1, 1024, 955]            --\n","│    └─BatchNorm1d: 2-36                 [1, 1024, 955]            (2,048)\n","│    └─ModuleList: 2-55                  --                        (recursive)\n","│    │    └─Conv1d: 3-18                 [1, 128, 955]             (49,280)\n","│    └─ReLU: 2-38                        [1, 128, 955]             --\n","│    └─ModuleList: 2-57                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-19            [1, 128, 955]             (256)\n","│    └─ModuleList: 2-55                  --                        (recursive)\n","│    │    └─Conv1d: 3-20                 [1, 128, 955]             (49,280)\n","│    └─ReLU: 2-41                        [1, 128, 955]             --\n","│    └─ModuleList: 2-57                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-21            [1, 128, 955]             (256)\n","│    └─ModuleList: 2-55                  --                        (recursive)\n","│    │    └─Conv1d: 3-22                 [1, 128, 955]             (49,280)\n","│    └─ReLU: 2-44                        [1, 128, 955]             --\n","│    └─ModuleList: 2-57                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-23            [1, 128, 955]             (256)\n","│    └─ModuleList: 2-55                  --                        (recursive)\n","│    │    └─Conv1d: 3-24                 [1, 128, 955]             (49,280)\n","│    └─ReLU: 2-47                        [1, 128, 955]             --\n","│    └─ModuleList: 2-57                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-25            [1, 128, 955]             (256)\n","│    └─ModuleList: 2-55                  --                        (recursive)\n","│    │    └─Conv1d: 3-26                 [1, 128, 955]             (49,280)\n","│    └─ReLU: 2-50                        [1, 128, 955]             --\n","│    └─ModuleList: 2-57                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-27            [1, 128, 955]             (256)\n","│    └─ModuleList: 2-55                  --                        (recursive)\n","│    │    └─Conv1d: 3-28                 [1, 128, 955]             (49,280)\n","│    └─ReLU: 2-53                        [1, 128, 955]             --\n","│    └─ModuleList: 2-57                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-29            [1, 128, 955]             (256)\n","│    └─ModuleList: 2-55                  --                        (recursive)\n","│    │    └─Conv1d: 3-30                 [1, 128, 955]             (49,280)\n","│    └─ReLU: 2-56                        [1, 128, 955]             --\n","│    └─ModuleList: 2-57                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-31            [1, 128, 955]             (256)\n","│    └─Conv1d: 2-58                      [1, 1024, 955]            (1,049,600)\n","│    └─ReLU: 2-59                        [1, 1024, 955]            --\n","│    └─BatchNorm1d: 2-60                 [1, 1024, 955]            (2,048)\n","│    └─MaxPool1d: 2-61                   [1, 1024, 318]            --\n","│    └─AFMS: 2-62                        [1, 1024, 318]            1,024\n","│    │    └─Linear: 3-32                 [1, 1024]                 (1,049,600)\n","│    │    └─Sigmoid: 3-33                [1, 1024]                 --\n","├─MaxPool1d: 1-5                         [1, 1024, 318]            --\n","├─Bottle2neck: 1-6                       [1, 1024, 318]            --\n","│    └─Identity: 2-63                    [1, 1024, 318]            --\n","│    └─Conv1d: 2-64                      [1, 1024, 318]            (1,049,600)\n","│    └─ReLU: 2-65                        [1, 1024, 318]            --\n","│    └─BatchNorm1d: 2-66                 [1, 1024, 318]            (2,048)\n","│    └─ModuleList: 2-85                  --                        (recursive)\n","│    │    └─Conv1d: 3-34                 [1, 128, 318]             (49,280)\n","│    └─ReLU: 2-68                        [1, 128, 318]             --\n","│    └─ModuleList: 2-87                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-35            [1, 128, 318]             (256)\n","│    └─ModuleList: 2-85                  --                        (recursive)\n","│    │    └─Conv1d: 3-36                 [1, 128, 318]             (49,280)\n","│    └─ReLU: 2-71                        [1, 128, 318]             --\n","│    └─ModuleList: 2-87                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-37            [1, 128, 318]             (256)\n","│    └─ModuleList: 2-85                  --                        (recursive)\n","│    │    └─Conv1d: 3-38                 [1, 128, 318]             (49,280)\n","│    └─ReLU: 2-74                        [1, 128, 318]             --\n","│    └─ModuleList: 2-87                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-39            [1, 128, 318]             (256)\n","│    └─ModuleList: 2-85                  --                        (recursive)\n","│    │    └─Conv1d: 3-40                 [1, 128, 318]             (49,280)\n","│    └─ReLU: 2-77                        [1, 128, 318]             --\n","│    └─ModuleList: 2-87                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-41            [1, 128, 318]             (256)\n","│    └─ModuleList: 2-85                  --                        (recursive)\n","│    │    └─Conv1d: 3-42                 [1, 128, 318]             (49,280)\n","│    └─ReLU: 2-80                        [1, 128, 318]             --\n","│    └─ModuleList: 2-87                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-43            [1, 128, 318]             (256)\n","│    └─ModuleList: 2-85                  --                        (recursive)\n","│    │    └─Conv1d: 3-44                 [1, 128, 318]             (49,280)\n","│    └─ReLU: 2-83                        [1, 128, 318]             --\n","│    └─ModuleList: 2-87                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-45            [1, 128, 318]             (256)\n","│    └─ModuleList: 2-85                  --                        (recursive)\n","│    │    └─Conv1d: 3-46                 [1, 128, 318]             (49,280)\n","│    └─ReLU: 2-86                        [1, 128, 318]             --\n","│    └─ModuleList: 2-87                  --                        (recursive)\n","│    │    └─BatchNorm1d: 3-47            [1, 128, 318]             (256)\n","│    └─Conv1d: 2-88                      [1, 1024, 318]            (1,049,600)\n","│    └─ReLU: 2-89                        [1, 1024, 318]            --\n","│    └─BatchNorm1d: 2-90                 [1, 1024, 318]            (2,048)\n","│    └─AFMS: 2-91                        [1, 1024, 318]            1,024\n","│    │    └─Linear: 3-48                 [1, 1024]                 (1,049,600)\n","│    │    └─Sigmoid: 3-49                [1, 1024]                 --\n","├─MaxPool1d: 1-7                         [1, 1024, 318]            --\n","├─Conv1d: 1-8                            [1, 1536, 318]            (4,720,128)\n","├─ReLU: 1-9                              [1, 1536, 318]            --\n","├─Sequential: 1-10                       [1, 1536, 318]            --\n","│    └─Conv1d: 2-92                      [1, 128, 318]             (589,952)\n","│    └─ReLU: 2-93                        [1, 128, 318]             --\n","│    └─BatchNorm1d: 2-94                 [1, 128, 318]             (256)\n","│    └─Conv1d: 2-95                      [1, 1536, 318]            (198,144)\n","│    └─Softmax: 2-96                     [1, 1536, 318]            --\n","├─BatchNorm1d: 1-11                      [1, 3072]                 (6,144)\n","├─Linear: 1-12                           [1, 256]                  (786,688)\n","├─Sequential: 1-13                       [1, 256]                  --\n","│    └─Linear: 2-97                      [1, 256]                  65,792\n","│    └─Linear: 2-98                      [1, 256]                  65,792\n","│    └─Linear: 2-99                      [1, 256]                  65,792\n","==========================================================================================\n","Total params: 16,477,698\n","Trainable params: 197,376\n","Non-trainable params: 16,280,322\n","Total mult-adds (G): 14.03\n","==========================================================================================\n","Input size (MB): 0.19\n","Forward/backward pass size (MB): 332.91\n","Params size (MB): 65.89\n","Estimated Total Size (MB): 399.00\n","=========================================================================================="]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"PTOriGuehoo7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qmk1rtkZhomZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python ./trainSpeakerNet.py --config ./configs/RawNet3_AAM.yaml --train_list ../../dataset/zalo_dataset/dataset_fix/train_list.txt --train_path ../../dataset/zalo_dataset/dataset_fix/train --test_list ../../dataset/zalo_dataset/dataset_fix/veri_val.txt --test_path ../../dataset/zalo_dataset/dataset_fix/val --initial_model ../rawnet3/model.pt --max_epoch 20"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oz3pevSE4DHu","executionInfo":{"status":"ok","timestamp":1681302535900,"user_tz":-420,"elapsed":10843147,"user":{"displayName":"company HeFFE","userId":"04261338004278305890"}},"outputId":"b9179624-49a3-461a-f4ae-b4f06d97ae87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python Version: 3.9.16 (main, Dec  7 2022, 01:11:51) \n","[GCC 9.4.0]\n","PyTorch Version: 2.0.0+cu118\n","Number of GPUs: 1\n","Save path: exps/RawNet3_AAM\n","self.encoder_type ECA\n","Initialised AAMSoftmax margin 0.100 scale 30.000\n","Initialised Adam optimizer\n","Initialised step LR scheduler\n","Model ../rawnet3/model.pt loaded!\n","Processing 8000 of 8000:Loss 3.519989 TEER/TAcc 44.475% - 4.94 Hz \n"," 2023-04-12 09:59:40 Epoch 1, TEER/TAcc 44.48, TLOSS 3.519989, LR 0.001000\n","Reading 900 of 973: 6.24 Hz, embedding size 256\n","Computing 19900 of 20000: 3069.37 Hz\n"," 2023-04-12 10:02:22 Epoch 1, VEER 5.4000, MinDCF 0.33780\n","Processing 8064 of 8064:Loss 0.897120 TEER/TAcc 81.448% - 26.86 Hz \n"," 2023-04-12 10:09:03 Epoch 2, TEER/TAcc 81.45, TLOSS 0.897120, LR 0.001000\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3108.15 Hz\n"," 2023-04-12 10:11:43 Epoch 2, VEER 4.6400, MinDCF 0.31150\n","Processing 8032 of 8032:Loss 0.638218 TEER/TAcc 86.417% - 27.04 Hz \n"," 2023-04-12 10:16:46 Epoch 3, TEER/TAcc 86.42, TLOSS 0.638218, LR 0.001000\n","Reading 900 of 973: 6.34 Hz, embedding size 256\n","Computing 19900 of 20000: 3116.28 Hz\n"," 2023-04-12 10:19:26 Epoch 3, VEER 4.7400, MinDCF 0.30100\n","Processing 8000 of 8000:Loss 0.544260 TEER/TAcc 88.075% - 27.01 Hz \n"," 2023-04-12 10:24:23 Epoch 4, TEER/TAcc 88.08, TLOSS 0.544260, LR 0.001000\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3688.52 Hz\n"," 2023-04-12 10:27:02 Epoch 4, VEER 4.8000, MinDCF 0.31810\n","Processing 8032 of 8032:Loss 0.494940 TEER/TAcc 88.259% - 27.01 Hz \n"," 2023-04-12 10:32:01 Epoch 5, TEER/TAcc 88.26, TLOSS 0.494940, LR 0.001000\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3137.45 Hz\n"," 2023-04-12 10:34:41 Epoch 5, VEER 5.8200, MinDCF 0.31600\n","Processing 8000 of 8000:Loss 0.462276 TEER/TAcc 88.050% - 26.82 Hz \n"," 2023-04-12 10:39:37 Epoch 6, TEER/TAcc 88.05, TLOSS 0.462276, LR 0.001000\n","Reading 900 of 973: 6.34 Hz, embedding size 256\n","Computing 19900 of 20000: 3576.43 Hz\n"," 2023-04-12 10:42:16 Epoch 6, VEER 5.9700, MinDCF 0.33740\n","Processing 8032 of 8032:Loss 0.451386 TEER/TAcc 87.973% - 27.23 Hz \n"," 2023-04-12 10:47:14 Epoch 7, TEER/TAcc 87.97, TLOSS 0.451386, LR 0.001000\n","Reading 900 of 973: 6.35 Hz, embedding size 256\n","Computing 19900 of 20000: 3443.46 Hz\n"," 2023-04-12 10:49:53 Epoch 7, VEER 5.3700, MinDCF 0.34270\n","Processing 8032 of 8032:Loss 0.421405 TEER/TAcc 88.683% - 26.81 Hz \n"," 2023-04-12 10:54:50 Epoch 8, TEER/TAcc 88.68, TLOSS 0.421405, LR 0.001000\n","Reading 900 of 973: 6.34 Hz, embedding size 256\n","Computing 19900 of 20000: 3454.22 Hz\n"," 2023-04-12 10:57:29 Epoch 8, VEER 5.4000, MinDCF 0.31490\n","Processing 8032 of 8032:Loss 0.394669 TEER/TAcc 88.720% - 27.18 Hz \n"," 2023-04-12 11:02:27 Epoch 9, TEER/TAcc 88.72, TLOSS 0.394669, LR 0.001000\n","Reading 900 of 973: 6.34 Hz, embedding size 256\n","Computing 19900 of 20000: 3503.27 Hz\n"," 2023-04-12 11:05:06 Epoch 9, VEER 5.3300, MinDCF 0.32960\n","Processing 8032 of 8032:Loss 0.363003 TEER/TAcc 89.903% - 27.00 Hz \n"," 2023-04-12 11:10:03 Epoch 10, TEER/TAcc 89.90, TLOSS 0.363003, LR 0.001000\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3376.53 Hz\n"," 2023-04-12 11:12:42 Epoch 10, VEER 4.9800, MinDCF 0.33360\n","Processing 8032 of 8032:Loss 0.280852 TEER/TAcc 91.472% - 27.17 Hz \n"," 2023-04-12 11:17:41 Epoch 11, TEER/TAcc 91.47, TLOSS 0.280852, LR 0.000750\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3240.88 Hz\n"," 2023-04-12 11:20:21 Epoch 11, VEER 5.0400, MinDCF 0.30810\n","Processing 8032 of 8032:Loss 0.222614 TEER/TAcc 92.567% - 26.99 Hz \n"," 2023-04-12 11:25:19 Epoch 12, TEER/TAcc 92.57, TLOSS 0.222614, LR 0.000750\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3635.39 Hz\n"," 2023-04-12 11:27:58 Epoch 12, VEER 5.4500, MinDCF 0.33110\n","Processing 8096 of 8096:Loss 0.226395 TEER/TAcc 92.675% - 26.90 Hz \n"," 2023-04-12 11:32:57 Epoch 13, TEER/TAcc 92.68, TLOSS 0.226395, LR 0.000750\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3118.61 Hz\n"," 2023-04-12 11:35:37 Epoch 13, VEER 4.9100, MinDCF 0.31790\n","Processing 8000 of 8000:Loss 0.207562 TEER/TAcc 93.250% - 27.02 Hz \n"," 2023-04-12 11:40:34 Epoch 14, TEER/TAcc 93.25, TLOSS 0.207562, LR 0.000750\n","Reading 900 of 973: 6.34 Hz, embedding size 256\n","Computing 19900 of 20000: 3618.10 Hz\n"," 2023-04-12 11:43:12 Epoch 14, VEER 6.3300, MinDCF 0.38930\n","Processing 8032 of 8032:Loss 0.222369 TEER/TAcc 92.978% - 26.88 Hz \n"," 2023-04-12 11:48:10 Epoch 15, TEER/TAcc 92.98, TLOSS 0.222369, LR 0.000750\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3387.23 Hz\n"," 2023-04-12 11:50:49 Epoch 15, VEER 5.4100, MinDCF 0.35520\n","Processing 8032 of 8032:Loss 0.257785 TEER/TAcc 92.380% - 26.95 Hz \n"," 2023-04-12 11:55:47 Epoch 16, TEER/TAcc 92.38, TLOSS 0.257785, LR 0.000750\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3476.42 Hz\n"," 2023-04-12 11:58:26 Epoch 16, VEER 5.5100, MinDCF 0.37610\n","Processing 8032 of 8032:Loss 0.304119 TEER/TAcc 91.123% - 27.10 Hz \n"," 2023-04-12 12:03:23 Epoch 17, TEER/TAcc 91.12, TLOSS 0.304119, LR 0.000750\n","Reading 900 of 973: 6.32 Hz, embedding size 256\n","Computing 19900 of 20000: 3372.30 Hz\n"," 2023-04-12 12:06:03 Epoch 17, VEER 5.8900, MinDCF 0.38340\n","Processing 8032 of 8032:Loss 0.261135 TEER/TAcc 92.331% - 26.91 Hz \n"," 2023-04-12 12:11:00 Epoch 18, TEER/TAcc 92.33, TLOSS 0.261135, LR 0.000750\n","Reading 900 of 973: 6.34 Hz, embedding size 256\n","Computing 19900 of 20000: 3377.71 Hz\n"," 2023-04-12 12:13:40 Epoch 18, VEER 8.2300, MinDCF 0.42780\n","Processing 8064 of 8064:Loss 0.225139 TEER/TAcc 93.502% - 27.01 Hz \n"," 2023-04-12 12:18:38 Epoch 19, TEER/TAcc 93.50, TLOSS 0.225139, LR 0.000750\n","Reading 900 of 973: 6.32 Hz, embedding size 256\n","Computing 19900 of 20000: 3167.32 Hz\n"," 2023-04-12 12:21:18 Epoch 19, VEER 4.8100, MinDCF 0.31630\n","Processing 8000 of 8000:Loss 0.202660 TEER/TAcc 93.550% - 27.04 Hz \n"," 2023-04-12 12:26:14 Epoch 20, TEER/TAcc 93.55, TLOSS 0.202660, LR 0.000750\n","Reading 900 of 973: 6.33 Hz, embedding size 256\n","Computing 19900 of 20000: 3495.26 Hz\n"," 2023-04-12 12:28:54 Epoch 20, VEER 5.4200, MinDCF 0.33950\n"]}]},{"cell_type":"code","source":["print(\"hi\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BckHqcQmoqIX","executionInfo":{"status":"ok","timestamp":1680704651675,"user_tz":-420,"elapsed":3,"user":{"displayName":"Nhattoan Do","userId":"01443387595928657519"}},"outputId":"2acbfdc7-fba8-451d-cede-72218a35715f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hi\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mjIUTKyHKsHc"},"execution_count":null,"outputs":[]}]}